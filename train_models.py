#!/usr/bin/env python
"""
PHASE 1 & 2: Data Preparation + ML Model Training
ML Gateway DDoS Detection System

This script:
1. Loads KDD21+ dataset (148K samples)
2. Normalizes and prepares features (41 attributes)
3. Trains Random Forest and XGBoost classifiers
4. Evaluates performance (target: 94%+ accuracy)
5. Saves best model for gateway integration
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import xgboost as xgb
import pickle
import time
from pathlib import Path
import json

print("=" * 90)
print("ML GATEWAY - DATA PREPARATION & MODEL TRAINING")
print("=" * 90)

# Configuration
DATA_DIR = Path("data")
MODEL_DIR = Path("models")
TRAIN_FILE = DATA_DIR / "KDDTrain+.csv"
TEST_FILE = DATA_DIR / "KDDTest+.csv"

# Create models directory if it doesn't exist
MODEL_DIR.mkdir(exist_ok=True)

print("\nðŸ“Š PHASE 1: DATA PREPARATION")
print("-" * 90)

# 1. Load datasets
print("\n1. Loading KDD21+ datasets...")
start_load = time.time()

# KDD features (41 total - last column is label)
# Features: duration, protocol_type, service, flag, src_bytes, dst_bytes, land, wrong_fragment,
# urgent, hot, num_failed_logins, logged_in, num_compromised, root_shell, su_attempted, num_root,
# num_file_creations, num_shells, num_access_files, num_outbound_cmds, is_host_login, is_guest_login,
# count, srv_count, serror_rate, srv_serror_rate, rerror_rate, srv_rerror_rate, same_srv_rate,
# diff_srv_rate, srv_diff_host_rate, dst_host_count, dst_host_srv_count, dst_host_same_srv_rate,
# dst_host_diff_srv_rate, dst_host_same_src_port_rate, dst_host_srv_diff_host_rate, dst_host_serror_rate,
# dst_host_srv_serror_rate, dst_host_rerror_rate, dst_host_srv_rerror_rate, difficulty_level

try:
    # Read training data
    df_train = pd.read_csv(TRAIN_FILE, header=None)
    print(f"   âœ“ Training data: {len(df_train):,} samples loaded")
    
    # Read test data
    df_test = pd.read_csv(TEST_FILE, header=None)
    print(f"   âœ“ Test data: {len(df_test):,} samples loaded")
    
    load_time = time.time() - start_load
    print(f"   Load time: {load_time:.2f}s")
    
except Exception as e:
    print(f"   âŒ Error loading data: {e}")
    exit(1)

# 2. Data exploration
print("\n2. Data exploration...")
print(f"   Training shape: {df_train.shape}")
print(f"   Test shape: {df_test.shape}")
print(f"   Features: {df_train.shape[1] - 1} (41 network features + 1 difficulty level + label)")

# Last column is difficulty level, second to last is label
label_col = df_train.shape[1] - 2  # The actual label column
print(f"\n   Label distribution (Training):")
label_dist = df_train[label_col].value_counts()
for label, count in label_dist.head(10).items():
    pct = 100 * count / len(df_train)
    print(f"     {str(label):20s}: {count:7,} samples ({pct:5.1f}%)")

# 3. Prepare features and labels
print("\n3. Preparing features and labels...")
start_prep = time.time()

# Separate features and labels
X_train_raw = df_train.iloc[:, :-2]  # All columns except last two
y_train = df_train.iloc[:, label_col].values  # The label column

X_test_raw = df_test.iloc[:, :-2]
y_test = df_test.iloc[:, label_col].values

# Encode categorical columns (protocol_type=1, service=2, flag=3)
from sklearn.preprocessing import LabelEncoder as LE

# Protocol type (column 1)
le_protocol = LE()
X_train_raw.iloc[:, 1] = le_protocol.fit_transform(X_train_raw.iloc[:, 1].astype(str))
X_test_raw.iloc[:, 1] = le_protocol.transform(X_test_raw.iloc[:, 1].astype(str))

# Service (column 2)
le_service = LE()
X_train_raw.iloc[:, 2] = le_service.fit_transform(X_train_raw.iloc[:, 2].astype(str))
X_test_raw.iloc[:, 2] = le_service.transform(X_test_raw.iloc[:, 2].astype(str))

# Flag (column 3)
le_flag = LE()
X_train_raw.iloc[:, 3] = le_flag.fit_transform(X_train_raw.iloc[:, 3].astype(str))
X_test_raw.iloc[:, 3] = le_flag.transform(X_test_raw.iloc[:, 3].astype(str))

# Convert to numpy arrays
X_train = X_train_raw.values.astype(float)
X_test = X_test_raw.values.astype(float)

print(f"   Training features shape: {X_train.shape}")
print(f"   Test features shape: {X_test.shape}")

# 4. Encode labels to binary classification (Normal vs Attack)
print("\n4. Converting to binary classification...")
print("   Mapping: 'normal' â†’ Normal (0), Others â†’ Attack (1)")

# 'normal' is the benign class, others are attacks
y_train_binary = np.where(y_train == 'normal', 0, 1)
y_test_binary = np.where(y_test == 'normal', 0, 1)

train_normal = np.sum(y_train_binary == 0)
train_attack = np.sum(y_train_binary == 1)
test_normal = np.sum(y_test_binary == 0)
test_attack = np.sum(y_test_binary == 1)

print(f"   Training: {train_normal:,} normal, {train_attack:,} attacks")
print(f"   Test: {test_normal:,} normal, {test_attack:,} attacks")

# 5. Normalize features
print("\n5. Normalizing features (MinMax scaling to [0, 1])...")
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"   âœ“ Features scaled to [0, 1] range")
print(f"   Feature statistics:")
print(f"     Mean: {X_train_scaled.mean():.4f}")
print(f"     Std:  {X_train_scaled.std():.4f}")
print(f"     Min:  {X_train_scaled.min():.4f}")
print(f"     Max:  {X_train_scaled.max():.4f}")

prep_time = time.time() - start_prep
print(f"   Preparation time: {prep_time:.2f}s")

print("\n" + "=" * 90)
print("ðŸ“ˆ PHASE 2: ML MODEL TRAINING")
print("=" * 90)

# 6. Train Random Forest
print("\n1. Training Random Forest Classifier...")
start_rf = time.time()

rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1,
    verbose=0
)

rf_model.fit(X_train_scaled, y_train_binary)
rf_time = time.time() - start_rf

print(f"   âœ“ Training complete in {rf_time:.2f}s")

# Evaluate Random Forest
y_pred_rf = rf_model.predict(X_test_scaled)
y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

rf_accuracy = accuracy_score(y_test_binary, y_pred_rf)
rf_precision = precision_score(y_test_binary, y_pred_rf)
rf_recall = recall_score(y_test_binary, y_pred_rf)
rf_f1 = f1_score(y_test_binary, y_pred_rf)
rf_auc = roc_auc_score(y_test_binary, y_pred_proba_rf)

print(f"\n   Random Forest Performance:")
print(f"     Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)")
print(f"     Precision: {rf_precision:.4f}")
print(f"     Recall:    {rf_recall:.4f}")
print(f"     F1-Score:  {rf_f1:.4f}")
print(f"     ROC-AUC:   {rf_auc:.4f}")

# 7. Train XGBoost
print("\n2. Training XGBoost Classifier...")
start_xgb = time.time()

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=0
)

xgb_model.fit(X_train_scaled, y_train_binary)
xgb_time = time.time() - start_xgb

print(f"   âœ“ Training complete in {xgb_time:.2f}s")

# Evaluate XGBoost
y_pred_xgb = xgb_model.predict(X_test_scaled)
y_pred_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]

xgb_accuracy = accuracy_score(y_test_binary, y_pred_xgb)
xgb_precision = precision_score(y_test_binary, y_pred_xgb)
xgb_recall = recall_score(y_test_binary, y_pred_xgb)
xgb_f1 = f1_score(y_test_binary, y_pred_xgb)
xgb_auc = roc_auc_score(y_test_binary, y_pred_proba_xgb)

print(f"\n   XGBoost Performance:")
print(f"     Accuracy:  {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)")
print(f"     Precision: {xgb_precision:.4f}")
print(f"     Recall:    {xgb_recall:.4f}")
print(f"     F1-Score:  {xgb_f1:.4f}")
print(f"     ROC-AUC:   {xgb_auc:.4f}")

# 8. Ensemble voting
print("\n3. Ensemble Voting (RF + XGBoost)...")

# Soft voting (average probabilities)
ensemble_proba = (y_pred_proba_rf + y_pred_proba_xgb) / 2
y_pred_ensemble = (ensemble_proba >= 0.5).astype(int)

ens_accuracy = accuracy_score(y_test_binary, y_pred_ensemble)
ens_precision = precision_score(y_test_binary, y_pred_ensemble)
ens_recall = recall_score(y_test_binary, y_pred_ensemble)
ens_f1 = f1_score(y_test_binary, y_pred_ensemble)
ens_auc = roc_auc_score(y_test_binary, ensemble_proba)

print(f"   Ensemble Performance:")
print(f"     Accuracy:  {ens_accuracy:.4f} ({ens_accuracy*100:.2f}%)")
print(f"     Precision: {ens_precision:.4f}")
print(f"     Recall:    {ens_recall:.4f}")
print(f"     F1-Score:  {ens_f1:.4f}")
print(f"     ROC-AUC:   {ens_auc:.4f}")

# 9. Select best model
print("\n4. Model Comparison & Selection...")
models_comparison = {
    'Random Forest': {'accuracy': rf_accuracy, 'model': rf_model, 'type': 'rf'},
    'XGBoost': {'accuracy': xgb_accuracy, 'model': xgb_model, 'type': 'xgb'},
    'Ensemble': {'accuracy': ens_accuracy, 'model': None, 'type': 'ensemble'}
}

print("\n   Model Comparison:")
print("   " + "-" * 50)
for name, metrics in models_comparison.items():
    print(f"   {name:15s}: {metrics['accuracy']*100:6.2f}% accuracy")
    if metrics['accuracy'] == max(m['accuracy'] for m in models_comparison.values()):
        print(f"   {'â†’ SELECTED':15s}")

best_model_name = max(models_comparison.items(), key=lambda x: x[1]['accuracy'])[0]
best_model = models_comparison[best_model_name]['model']

print(f"\n   âœ… Best model: {best_model_name}")

# 10. Save models
print("\n5. Saving models...")

# Save the best model
if best_model_name == 'Random Forest':
    model_path = MODEL_DIR / 'best_model.pkl'
    pickle.dump(rf_model, open(model_path, 'wb'))
    scaler_path = MODEL_DIR / 'scaler.pkl'
    pickle.dump(scaler, open(scaler_path, 'wb'))
    print(f"   âœ“ Saved: {model_path}")
    
elif best_model_name == 'XGBoost':
    model_path = MODEL_DIR / 'best_model.pkl'
    pickle.dump(xgb_model, open(model_path, 'wb'))
    scaler_path = MODEL_DIR / 'scaler.pkl'
    pickle.dump(scaler, open(scaler_path, 'wb'))
    print(f"   âœ“ Saved: {model_path}")
    
else:  # Ensemble
    ensemble_data = {
        'rf_model': rf_model,
        'xgb_model': xgb_model,
        'type': 'ensemble'
    }
    model_path = MODEL_DIR / 'best_model.pkl'
    pickle.dump(ensemble_data, open(model_path, 'wb'))
    scaler_path = MODEL_DIR / 'scaler.pkl'
    pickle.dump(scaler, open(scaler_path, 'wb'))
    print(f"   âœ“ Saved: {model_path} (Ensemble)")

# Save metrics to JSON
metrics = {
    'best_model': best_model_name,
    'accuracy': float(models_comparison[best_model_name]['accuracy']),
    'models': {
        'random_forest': {
            'accuracy': float(rf_accuracy),
            'precision': float(rf_precision),
            'recall': float(rf_recall),
            'f1': float(rf_f1),
            'auc': float(rf_auc)
        },
        'xgboost': {
            'accuracy': float(xgb_accuracy),
            'precision': float(xgb_precision),
            'recall': float(xgb_recall),
            'f1': float(xgb_f1),
            'auc': float(xgb_auc)
        },
        'ensemble': {
            'accuracy': float(ens_accuracy),
            'precision': float(ens_precision),
            'recall': float(ens_recall),
            'f1': float(ens_f1),
            'auc': float(ens_auc)
        }
    },
    'dataset': {
        'training_samples': len(X_train),
        'test_samples': len(X_test),
        'features': X_train.shape[1],
        'training_normal': int(train_normal),
        'training_attacks': int(train_attack),
        'test_normal': int(test_normal),
        'test_attacks': int(test_attack)
    },
    'training_time': {
        'random_forest': float(rf_time),
        'xgboost': float(xgb_time)
    }
}

metrics_path = MODEL_DIR / 'model_metrics.json'
with open(metrics_path, 'w') as f:
    json.dump(metrics, f, indent=2)
print(f"   âœ“ Saved: {metrics_path}")

# 11. Detailed evaluation
print("\n6. Detailed Evaluation (Best Model: {})...".format(best_model_name))
print("\n   Confusion Matrix:")
if best_model_name == 'Ensemble':
    cm = confusion_matrix(y_test_binary, y_pred_ensemble)
    best_accuracy = ens_accuracy
else:
    y_pred_best = rf_model.predict(X_test_scaled) if best_model_name == 'Random Forest' else xgb_model.predict(X_test_scaled)
    cm = confusion_matrix(y_test_binary, y_pred_best)
    best_accuracy = models_comparison[best_model_name]['accuracy']

tn, fp, fn, tp = cm.ravel()
print(f"     True Negatives (TN):  {tn:7,}")
print(f"     False Positives (FP): {fp:7,}")
print(f"     False Negatives (FN): {fn:7,}")
print(f"     True Positives (TP):  {tp:7,}")

specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\n   Specificity (TN rate): {specificity:.4f}")
print(f"   Sensitivity (TP rate): {sensitivity:.4f}")

print("\n" + "=" * 90)
print("âœ… TRAINING COMPLETE")
print("=" * 90)

print(f"""
Summary:
  â€¢ Training samples: {len(X_train):,}
  â€¢ Test samples: {len(X_test):,}
  â€¢ Features: {X_train.shape[1]}
  â€¢ Best model: {best_model_name}
  â€¢ Accuracy: {best_accuracy*100:.2f}%
  
Status: Ready for Phase 3 (Gateway Integration)
  Models saved to: {MODEL_DIR}/
  Next: Run train_and_integrate.py to load model into gateway

Performance Summary:
  Random Forest:  {rf_accuracy*100:.2f}% accuracy
  XGBoost:        {xgb_accuracy*100:.2f}% accuracy  
  Ensemble:       {ens_accuracy*100:.2f}% accuracy
  
Expected Gateway Performance:
  â€¢ Detection Accuracy: {best_accuracy*100:.2f}%+
  â€¢ Latency: <50ms per request
  â€¢ Throughput: 1,200+ req/s
""")
